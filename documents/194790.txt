In agile like development, who should write test cases?
testing project-management agile testcase
Our team has a task system where we post small incremental tasks assigned to each developer. 
Each task is developed in its own branch, and then each branch is tested before being merged to the trunk.
My question is: Once the task is done, who should define the test cases that should be done on this task? 
Ideally I think the developer of the task himself is best suited for the job, but I have had a lot of resistance from developers who think it's a waste of their time, or that they simply don't like doing it. 
The reason I don't like having my QA people do it, is because I don't like the idea of them creating their own work.  For example they might leave out things that are simply too much work to test, and they may not know the technical detail that is needed.  
But likewise, the down part of developers doing the test cases, is that they may leave out things that they think will break. (even subconsciously maybe) 
As the project manager, I ended up writing the test cases for each task myself, but my time is taxed and I want to change this.
Suggestions?
EDIT: By test cases I mean the description of the individual QA tasks that should be done to the branch before it should be merged to the trunk. (Black Box)


We experimented with a pairing of the developer with a QA person with pretty good results.  They generally 'kept each other honest' and since the developer had unit tests to handle the code, s/he was quite intimate with the changes already.  The QA person wasn't but came at it from the black box side.  Both were held accountable for completeness.  Part of the ongoing review process helped to catch unit test shortcomings and so there weren't too many incidents that I was aware of where anyone was purposely avoiding writing X test because it would likely prove there was a problem.
I like the pairing idea in some instances and think it worked pretty well.  Might not always work, but having those players from different areas interact helped to avoid the 'throw it over the wall' mentality that often happens.
Anyhow, hope that is somehow helpful to you.

I think the Project Manager, or Business Analyst should write those test cases.
They should then hand them over to the QA person to flesh out and test.
That way you ensure no missing gaps between the spec, and what's actually tested and delivered.
The developer's should definately not do it, as they'll be testing their unit tests. 
So it's a waste of time. 
In addition these tests will find errors which the developer will never find as they are probably due to a misunderstanding in the spec, or a feature or route through the code not having been thought through and implemented correctly.
If you find you don't have enough time for this, hire someone else, or promote someone to this role, as it's key to delivering an excellent product.


The reason I don't like having my QA people do it, is because I don't like the idea of them creating their own work. For example they might leave out things that are simply too much work to test, and they may not know the technical detail that is needed.

Yikes, you need to have more trust in your QA department, or a  better one.  I mean, imagine of you had said "I don't like having my developers develop software.  I don't like the idea of them creating their own work."
As a developer, I Know that there are risks involved in writing my own tests.  That's not to say I don't do that (I do, especially if I am doing TDD) but I have no illusions about test coverage.  Developers are going to write tests that show that their code does what they think it does.  Not too many are going to write tests that apply to the actual business case at hand.
Testing is a skill, and hopefully your QA department, or at least, the leaders in that department, are well versed in that skill.  

"developers who think it's a waste of their time, or that they simply don't like doing it"  Then reward them for it.  What social engineering is necessary to get them to create test cases?
Can QA look over the code and test cases and pronounce "Not Enough Coverage -- Need More Cases".  If so, then the programmer that has "enough" coverage right away will be the Big Kahuna.  
So, my question is: Once the task is done, who should define the goal of "enough" test cases for this task?  Once you know "enough", you can make the programmers responsible for filling in "enough" and QA responsible for assuring that "enough" testing is done.
Too hard to define "enough"?  Interesting.   Probably this is the root cause of the conflict with the programmers in the first place.  They might feel it's a waste of their time because they already did "enough" and now someone is saying it isn't "enough".

Select (not just pick randomly) one or two testers, and let them write the test cases. Review. It could also be useful if a developer working with a task looks at the test cases for the task. Encourage testers to suggest improvements and additions to test sets - sometimes people are afraid to fix what the boss did. This way you might find someone who is good at test design.
Let the testers know about the technical details - I think everyone in an agile team should have read access to code, and whatever documentation is available. Most testers I know can read (and write) code, so they might find unit tests useful, possibly even extend them. Make sure the test designers get useful answers from the developers, if they need to know something.

From past experience, we had pretty good luck defining tests at different levels to test slightly different things:
1st tier: At the code/class level, developers should be writing atomic unit tests. The purpose is to test individual classes and methods as much as possible. These tests should be run by developers as they code, presumably before archiving code into source control, and by a continuous-integration server (automated) if one is being used.
2nd tier: At the component integration level, again have developers creating unit tests, but that test the integration between components. The purpose is not to test individual classes and components, but to test how they interact with each other. These tests should be run manually by an integration engineer, or automated by a continuous-integration seerver, if one is in use.
3rd tier: At the application level, have the QA team running their system tests.  These test cases should be based off the business assumptions or requirements documents provided by a product manager.  Basically, test as if you were an end user, doing the things end users should be able to do, as documented int eh requirements.  These test cases should be written by the QA team and the product managers who (presumably) know what the customer wants and how they are expected to use the application.
I feel this provides a pretty good level of coverage.  Of course, tiers 1 and 2 above should ideally be run before sending a built application to the QA team.
Of course, you can adapt this to whatever fits your business model, but this worked pretty well at my last job.  Our continous-integration server would kick out an email to the development team if one of the unit tests failed during the build/integration process too, incase someone forgot to run their tests and committed broken code into the source archive.

My suggestion would be to having someone else look over the test cases before the code is merged to ensure quality.  Granted this may mean that a developer is overlooking another developer's work but that second set of eyes may catch something that wasn't initially caught.  The initial test cases can be done by any developer, analyst or manager, not a tester.
QA shouldn't write the test cases as they may be situations where the expected result hasn't been defined and by this point, it may be hard to have someone referee between QA and development if each side thinks their interpretation is the right one.  It is something I have seen many many times and wish it didn't happen as often as it does.

I loosely break my tests down into "developer" tests and "customer" tests, the latter of which would be "acceptance tests".  The former are the tests that developers write to verify that their code is performing correctly.  The later are tests that someone other than developers write to ensure that behavior matches the spec.  The developers must never write the accepatance tests because their creation of the software they're testing assumes that they did the right thing.  Thus, their acceptance tests are probably going to assert what the developer already knew to be true.
The acceptance tests should be driven by the spec and if they're written by the developer, they'll get driven by the code and thus by the current behavior, not the desired behavior.

the QA people, in conjunction with the "customer", should define the test cases for each task [we're really mixing terminology here], and the developer should write them. first!

The Team.
If a defect gets to a customer, it is the team's fault, therefore the team should be writing test cases to assure that defects don't reach the customer.

The Project Manager (PM) should understand the domain better than anyone on the team.  Their domain knowledge is vital to having test cases that make sense with regard to the domain.  They will need to provide example inputs and answer questions about expectations on invalid inputs.  They need to provide at least the 'happy path' test case.
The Developer(s) will know the code.  You suggest the developer may be best for the task, but that you are looking for black box test cases.  Any tests that a developer comes up with are white box tests.  That is the advantage of having developers create test cases – they know where the seams in the code are.  
Good developers will also be coming to the PM with questions "What should happen when...?" – each of these is a test case.  If the answer is complex "If a then x, but if b then y, except on Thursdays" – there are multiple test cases.
The Testers (QA) know how to test software.  Testers are likely to come up with test cases that the PM and the developers would not think of – that is why you have testers.


The Agile canon is that you should have (at least) two layers of tests: developer tests and customer tests. 
Developer tests are written by the same people who write the production code, preferably using test driven development. They help coming up with a well decoupled design, and ensure that the code is doing what the developers think it is doing - even after a refactoring.
Customer tests are specified by the customer or customer proxy. They are, in fact, the specification of the system, and should be written in a way that they are both executable (fully automated) and understandable by the business people. Often enough, teams find ways for the customer to even write them, with the help of QA people. This should happen while - or even before - the functionality gets developed.
Ideally, the only tasks for QA to do just before the merge, is pressing a button to run all automated tests, and do some additional exploratory (=unscripted) testing. You'll want to run those tests again after the merge, too, to make sure that integrating the changes didn't break something.

A test case begins first in the story card. 
The purpose of testing is to drive defects to the left (earlier in the software development process when they are cheaper and faster to fix). 
Each story card should include acceptance criteria. The Product Owner pairs with the Solution Analyst to define the acceptance criteria for each story. This criteria is used to determine if a story card's purpose has been meet. 
The story card acceptance criteria will determine what automated unit tests need to be coded by the developers as they do Test Driven Development. It will also drive the automated functional test implemented by the autoamted testers (and perhaps with developer support if using tools like FIT). 
Just as importantly, the acceptance criteria will drive the automated performance tests and can be used when analyzing the profiling of the application by the developers. 
Finally, the user acceptance test will be determined by the acceptance criteria in the story cards and should be designed by the business partner and or users. Follow this process and you will likely release with zero defects.

I've rarely have heard of or seen Project Managers write test cases except for in the smaller teams.  In any large,complex software application have to have an analyst that really knows the application.  I worked at a mortgage company as a PM - was I to understand sub-prime lending, interest rates, and the such?   Maybe at a superficial level, but real experts needed to make sure those things worked.   My job was to keep the team healthy, protect the agile principles, and look for new opportunities for work for my team. 

The system analyst should review over all test-cases and its correct relation with the use-cases.
Plus the Analyst should perform the final UAT, which could be based on test-cases also.
So the analyst and the quality guy are making sort of peer-review.
The quality is reviewing the use-cases while he is building test-cases, and the analyst is reviewing the test-cases after they are written and while he is performing UAT.

Of course BA is the domain expert, not from technical point of view. BA understands the requirements and the test cases should be mapped to the requirements. Developers should not be the persons writing the test cases to test against their code. QA can write detail test steps per requirement. But the person who writes the requirement should dictate what needs to be tested. Who actually writes the test cases, I dont care too much as long as the test cases can be traced back to requirements. I would think it makes sense that BA guides the testing direction or scope, and QA writes the granular testing plans.

We need to evolve from the "this is how it has been done or should be done mentality" it is failing and failing continuously.  The best way to resolve the test plan/cases writing issue is that test cases should be written on the requirements doc in waterfall or the user story in agile as those reqs/user stories are being written.  This way there is no question what needs to be tested and QA and UAT teams can execute the test case(s) and focus time on actual testing and defect resolution.

The largest problem I have seen in Software is the lack of hashed Ideas. Un written requirements or loosely based requirement leave too much to the imagination and can often times lead to more work or more defects. A concrete statement of work leads to a concrete solution. 
When Anyone needs to ask an excessive number of questions defects should appear instantly. 
Shareholder Presents Design.
PM Creates requirements.
QA/DEV Review Requirement with PM,
QA Writes test passes it off to Dev for Review.
DEV Builds.
DEV Builds Integration test.
DEV Merges
QA Builds/Test
Dev Closes Sprint
PM Presents to Shareholder
Defects should lower based on the efficiency of the QA's asking the correct questions so that DEV can focus on completing their goals. 
The fewest amount of obstacles should appear to DEV at anytime. Out side of Technical debt. Requirements should be the least of their worries (But always is). They are taxed too heavily in most organizations (in my humble opinion.)
