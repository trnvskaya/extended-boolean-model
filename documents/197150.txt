Skip file lines until a match is found, then output the rest
unix text-processing
I can write a trivial script to do this but in my ongoing quest to get more familliar with unix I'd like to learn efficient methods using built in commands instead.
I need to deal with very large files that have a variable number of header lines. the last header line consists of the text 'LastHeaderLine'. I wish to output everything after this line. (I'm not worried about false positive matches.)


Using sed:
sed -ne '/LastHeaderLine/,$p' <inputfile

will match everything from the regex match to the end of the file. 'p' prints the lines that match.
Edit:
On second thought, you don't want to print the line matching LastHeaderLine. This is difficult to do with sed. In perl, you could do the following:
perl -ne 'if ($flag) {print;} if (/LastHeaderFile/) {$flag=1;}' <inputfile

This would print only lines strictly following the regex match.

Why not try awk for this? It would look like this:
awk 'NR == 1, /LastHeaderLine/ { next } { print }' myinputfile > myoutputfile

where NR == 1 is true for the first line, /LastHeaderLine/ matches your last header line. The comma operator lets the following function { next } fire for all sentences in the range of the two regular expression. In this case it will skip to the next line of input without further operation. For all other input lines it will print the lines to the standard output, which you can redirect using >.

Similar to the answer of Avi, but without including the line with "LastHeaderLine".
sed -e '1,/LastHeaderLine/d'

